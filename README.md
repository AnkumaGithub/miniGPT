# miniGPT
Здесь будет написана мини версия gpt (наверное от 20 до 80 млн параметров)    
Архитектура основана на gpt 2, но уже есть отклонения и их количество будет увеличиваться    
Обучаться будет на урезанной(где-то 1/8 часть) версии openwebtext на rtx 3060, 32 gb ram, intel i5(11 поколения)    
Архитектурные решения: ротационные поизиционные эмбеддинги, возможна замена LayerNorm на Dynamic Tanh.
