# miniGPT
Здесь будет написана мини версия gpt (наверное от 20 до 80 млн параметров)    
Архитектура основана на чем-то среднем между gpt 2 и LLaMa 2   
Обучаться будет на урезанной(где-то 1/8 часть) версии openwebtext на rtx 3060, 32 gb ram, intel i5(11 поколения)    
Архитектурные решения: ротационные поизиционные эмбеддинги, возможна замена LayerNorm на Dynamic Tanh, kv-кэширование.
